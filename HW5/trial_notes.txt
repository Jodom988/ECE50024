Overall notes:
    Was training naive_trial 1 and 2 with distorted/stretched images, for trial 3 fixed this
    Batch sizes should be < 32 and powers of 2. Was doing huuuge batches for 1-3, fixed with 4

    From cropping center/largest strats: Train missed 10.8337%, test missed 10.6557%
    From cropping single strat: Train missed 24.9515%, test missed 25.2013%

    From creating VGG embeddings: 
        Test: 768 (15%) had multiple faces found
        train_train_vgg: 8919 (16%) had multiple faces found
        train_test_vgg: 2245 (16%) had multiple faces found

        train_train_vgg_facenet512: 16% were not able to find single face
        train_test_vgg_facenet512: 16% were not able to find single face


deepface_embeddings_1:
    Had about 70 and got about 70

    Trained with lots of different batch sizes
    def get_model():
        model = models.Sequential()
        model.add(layers.Input(INPUT_VECTOR_SHAPE))
        model.add(layers.Dense(2048, activation='relu'))
        model.add(layers.Dense(100, activation='softmax'))

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

deepface_embeddings_2:
    Seemed like it was about the same as deepface_embeddings_1
    def get_model():
        model = models.Sequential()
        model.add(layers.Input(INPUT_VECTOR_SHAPE))
        model.add(layers.Dense(2048, activation='relu'))
        model.add(layers.Dense(1024, activation='relu'))
        model.add(layers.Dense(100, activation='softmax'))

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

deepface_embeddings_3

    def get_model():
        model = models.Sequential()
        model.add(layers.Input(INPUT_VECTOR_SHAPE))
        model.add(layers.Dense(100, activation='softmax'))

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

deepface_embeddings_4
    Used skip connections
    Did batch size of 64 until end and then did 1024
    def get_model():
        inputs = layers.Input(INPUT_VECTOR_SHAPE)
        x = layers.Dense(1024, activation='relu')(inputs)
        x = layers.Concatenate(axis=1)([inputs, x])
        y = layers.Dense(100, activation='softmax')(x)

        model = keras.Model(inputs, y)

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

deepface_embeddings_5:
    Same as deepface_embeddings_4 but used batch size of 2048

deepface_embeddings_6:
    Same as deepface_embeddings_5 but used activation of sigmoid

deepface_embeddings_7:
    Same as deepface_embeddings_6 but used activation of leakyrelu

deepface_embeddings_8:
    Used batch size of 4096
    def get_model():
    inputs = layers.Input(INPUT_VECTOR_SHAPE)
        x1 = layers.Dense(2048)(inputs)
        x2 = layers.LeakyReLU(alpha=0.05)(x1)
        x3 = layers.Concatenate(axis=1)([inputs, x2])
        x4 = layers.Dense(2048, activation='sigmoid')(x3)
        y = layers.Dense(100, activation='softmax')(x4)

        model = keras.Model(inputs, y)

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

deepface_embeddings_9:
    def get_model():
        inputs = layers.Input(INPUT_VECTOR_SHAPE)
        x1 = layers.Dense(2048)(inputs)
        x2 = layers.LeakyReLU(alpha=0.05)(x1)
        x3 = layers.Concatenate(axis=1)([inputs, x2])
        x4 = layers.Dense(2048, activation='sigmoid')(x3)
        x5 = layers.Concatenate(axis=1)([x4, x3])
        y = layers.Dense(100, activation='softmax')(x5)

        model = keras.Model(inputs, y)

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model
    
deepface_embeddings_10:
    def get_model():
        inputs = layers.Input(INPUT_VECTOR_SHAPE)
        x1 = layers.Dense(2048)(inputs)
        
        x2 = layers.LeakyReLU(alpha=0.05)(x1)
        x3 = layers.Concatenate(axis=1)([inputs, x2])
        
        x4 = layers.Dense(2048, activation='sigmoid')(x3)
        x5 = layers.Concatenate(axis=1)([x4, x3])
        
        x6 = layers.Dense(2048, activation='relu')(x5)
        x7 = layers.Concatenate(axis=1)([x6, x5])
        
        y = layers.Dense(100, activation='softmax')(x7)

        model = keras.Model(inputs, y)

        model.summary()
        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

predictions-deepface-vgg:
    Submitted and got ~52.1% accuracy
    Used deepface's VGGFace model with ~200 imgs/person in DB

predictions-deepface-facenet:
    Submitted and got ~60.0% accuracy
    Used deepface's Facenet model with every img/person in DB
    About 300 images were unknown, used VGG to predict those

predictions-deepface-facenet512:
    Submitted and got ~61.9% accuracy
    Used deepface's Facenet512 model with every img/person in DB

transfer_vgg_1:
    Done with cropped closest dataset
    def get_model():
        vggface_base = VGGFace(model='resnet50', include_top=False, input_shape=IM_SIZE)
        vggface_base.trainable = False

        last_layer = vggface_base.get_layer('avg_pool').output

        # # Build the model
        inputs = tf.keras.Input(shape=IM_SIZE)
        x = vggface_base(inputs)
        x = layers.Flatten(name='flatten')(x)
        out = layers.Dense(100, name='Classifier')(x)
        model = keras.Model(inputs, out)
        
        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])
        
        return model

transfer_vgg_2:
    Same as transfer 1, but with bigger model

transfer_vgg_3:
    Same as tranasfer_1

transfer_vgg_4:
    Same as transfer_1 but using cropped_largest dataset

transfer_vgg_5:
    Same as transfer_1 but using cropped_single dataset
    Best one yet!! Max accurracy of 36% with test dataset, submitted to Kaggle and got 29.6%

transfer_vgg_6:
    using cropped_single dataset
    Added activation function and dense hidden layer with relu activation
        WOW! This made a huge difference, this might be better than using the deepface models

    Max Accuracy of 79.14 (did not finish training, still improving when stopped)
    Need to submit predictions to Kaggle

    def get_model():
        vggface_base = VGGFace(model='resnet50', include_top=False, input_shape=IM_SIZE)
        vggface_base.trainable = False

        last_layer = vggface_base.get_layer('avg_pool').output

        # # Build the model
        inputs = tf.keras.Input(shape=IM_SIZE)
        x = vggface_base(inputs)
        x = layers.Flatten(name='flatten')(x)
        out = layers.Dense(1024, name='Classifier', activation='relu')(x)
        out = layers.Dense(N_LABELS, name='Classifier', activation='softmax')(x)
        model = keras.Model(inputs, out)
        
        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])
        
        return model

transfer_vgg_7:
    Tried with a larger model, didn't really have any changes
    Around 4500 batches, added training data that was distorted
    def get_model_vgg():
        vggface_base = VGGFace(model='resnet50', include_top=False, input_shape=IM_SIZE)
        vggface_base.trainable = False

        last_layer = vggface_base.get_layer('avg_pool').output

        # # Build the model
        inputs = tf.keras.Input(shape=IM_SIZE)
        x = vggface_base(inputs)
        x = layers.Flatten(name='flatten')(x)
        x = layers.Dense(2048, name='hidden1', activation='relu')(x)
        x = layers.Dense(1024, name='hidden2', activation='relu')(x)
        out = layers.Dense(N_LABELS, name='Classifier', activation='softmax')(x)
        model = keras.Model(inputs, out)
        
        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])
        
        return model

transfer_vgg_8:
    Using model from transfer_vgg_6, 19,144 batches have gone through, training ENTIRE model now
    

cropped_1:
    Using copped data for this run, doing 128x128 B/W images 

cropped_2:
    Using extended cropped data fro this run, doing 128x128 B/W images

    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(512, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        
        model.add(layers.Dense(2048, activation='relu'))
        model.add(layers.Dense(1024, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

naive_trial1:
    Used 128x128 B/W image
    Used batch size of 1024, had an error where it ran out of memory

    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

naive_trial2:
    Used 256x256 BW image
    Used batch size of 256

    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(64, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

naive_trial3:
    Used 256x256 BW image
    Used batch size of 1024
    
    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model 

naive_trial4:
    Same as trial 3 but with batch size of 64

naive_trial5:
    Continuation of naive_trial4, but with batch size of 32

naive_trial6:
    def get_model():
    model = models.Sequential()
    model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=IM_SIZE))
    model.add(layers.MaxPooling2D((3, 3)))
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((3, 3)))

    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(100, activation='relu'))

    model.summary()

    model.compile(optimizer='adam',
            loss=tf.keras.losses.CategoricalCrossentropy(),
            metrics=['accuracy'])

    return model 

naive_trial7:
    Used batch size of 32
    
    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model 

naive_trial8:
    batch size of 16
    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=IM_SIZE))
        model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(1024, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model 

naive_trial9:
    Made samples come in same order as in CSV file (CSV file already random)
    Retrained model on samples it's already seen to avoid catestrophic forgetting (didn't work)
    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=IM_SIZE))
        model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(1024, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

naive_trial10:
    Went back to image size of 128x128
    Training on samples model has already seen to avoid catestrophic forgetting
    Using model from trial 1
    batch size of 32
    def get_model():
        model = models.Sequential()
        model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=IM_SIZE))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(128, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))
        model.add(layers.Conv2D(256, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((3, 3)))

        model.add(layers.Flatten())
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dense(100, activation='relu'))

        model.summary()

        model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])

        return model

